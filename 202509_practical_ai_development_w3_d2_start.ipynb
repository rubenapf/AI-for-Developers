{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenapf/AI-for-Developers/blob/main/202509_practical_ai_development_w3_d2_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - v2\n",
        "\n",
        "- Same data as v1 (Bitcoin whitepaper)\n",
        "- LLM answer generation using LangChain chain\n",
        "- Gradio ChatInterface\n",
        "- User experience (natural language answers) using also an output parser"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# v2 - added gradio\n",
        "#--------------------------------------------------------------------------------\n",
        "%pip install gradio -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "\n",
        "# Version Management\n",
        "VERSION = \"v2\"\n",
        "# NOTE: We re-ingest to keep versions clean and independent,\n",
        "#       but technically v2 could use v1's data since structure is identical\n",
        "\n",
        "# Vector Database collection name\n",
        "COLLECTION_NAME = f\"bitcoin_docs_{VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# v2 - added ChatOpenAI\n",
        "#--------------------------------------------------------------------------------\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Vector Database\n",
        "vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=\"code_for_all_rag\"\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingest_documents():\n",
        "    \"\"\"\n",
        "    INGESTION PIPELINE - VERSION v2\n",
        "\n",
        "    DATA: Identical to v1\n",
        "    WHY RE-INGEST? Keep versions independent for teaching clarity and comparison\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"STARTING INGESTION PIPELINE - VERSION {VERSION}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: LOAD DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(\"\\n[1/3] Loading file from URL...\")\n",
        "\n",
        "    loader = PyPDFLoader(\"https://bitcoin.org/bitcoin.pdf\")\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"✓ Loaded {len(documents)} pages from file\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: CHUNK DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/3] Chunking documents...\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"✓ Split into {len(chunks)} chunks\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: CREATE EMBEDDINGS AND STORE IN CHROMA\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[3/3] Creating embeddings and storing in Chroma...\")\n",
        "    print(f\"  Collection name: {COLLECTION_NAME}\")\n",
        "\n",
        "    vectorstore.add_documents(\n",
        "        documents=chunks,\n",
        "        ids=[str(i) for i in range(len(chunks))]\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Embeddings created and stored\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPCOgLJCgHiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Ingestion"
      ],
      "metadata": {
        "id": "ZSnvBHEnlha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingest_documents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHP6fpIzlj9B",
        "outputId": "f5c9f3da-30f4-4d5e-9bf6-d33ac43cd374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "STARTING INGESTION PIPELINE - VERSION v2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/3] Loading file from URL...\n",
            "✓ Loaded 9 pages from file\n",
            "\n",
            "[2/3] Chunking documents...\n",
            "✓ Split into 30 chunks\n",
            "\n",
            "[3/3] Creating embeddings and storing in Chroma...\n",
            "  Collection name: bitcoin_docs_v2\n",
            "✓ Embeddings created and stored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#--------------------------------------------------------------------------------\n",
        "# v2 - added ChatPromptTemplate and StrOutputParser imports\n",
        "#--------------------------------------------------------------------------------\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def inference(query: str) -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE PIPELINE - VERSION v2\n",
        "\n",
        "    WHAT CHANGED FROM v1?\n",
        "    - Return type: List[Document] → str (natural language answer)\n",
        "    - Added: Context formatting\n",
        "    - Added: Prompt engineering\n",
        "    - Added: LLM generation\n",
        "\n",
        "    THE RAG FLOW:\n",
        "    1. Retrieve: Get relevant documents (same as v1)\n",
        "    2. Format: Combine documents into context string\n",
        "    3. Prompt: Create structured instruction for LLM\n",
        "    4. Chain: prompt -> llm -> output parser\n",
        "    5. Generate: LLM produces natural language answer\n",
        "    6. Return: User gets readable answer\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer (v1 returned List[Document])\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: SIMILARITY SEARCH\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[1/5] Performing similarity search...\")\n",
        "    print(f\"  Query: '{query}'\")\n",
        "\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    print(f\"✓ Found {len(results)} relevant chunks\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: FORMAT CONTEXT (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[2/5] Formatting context for LLM...\")\n",
        "\n",
        "    # INSPECT: What does formatted context look like?\n",
        "    print(f\"\\nFirst 1500 chars: {context[:1500]}...\")\n",
        "    print(f\"✓ Context formatted ({len(context)} characters)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: PROMPT TEMPLATE (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # ChatPromptTemplate:\n",
        "    # - Defines structure with variables in {curly braces}\n",
        "    # - Variables are filled when chain is invoked\n",
        "    # - Reusable across all queries\n",
        "\n",
        "    print(\"\\n[3/5] Creating prompt template...\")\n",
        "    print(\"  Variables: {context}, {query}\")\n",
        "\n",
        "    # >>>> add code here\n",
        "\n",
        "    print(\"✓ Prompt template created\")\n",
        "    print(\"  This template will be reused for every query\")\n",
        "    print(\"  Variables will be filled automatically by the chain\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: COMPOSE CHAIN (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # The pipe (|) operator connects components (output from the last is the input of the next)\n",
        "    # Read left to right: prompt → llm → parser\n",
        "\n",
        "    # StrOutputParser:\n",
        "    # - LLMs return AIMessage objects (complex)\n",
        "    # - Parser extracts just the string content\n",
        "    # - Clean string output for users\n",
        "\n",
        "    print(\"\\n[4/5] Composing chain...\")\n",
        "\n",
        "    # >>>> add code here\n",
        "\n",
        "    print(\"\\n✓ Chain composed!\")\n",
        "    print(\"\\n  Chain structure:\")\n",
        "    print(\"  prompt_template  (formats variables) -> llm (generates response) -> output_parser (extracts string)\")\n",
        "    print(\"  Returns: String (natural language answer)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: GENERATE ANSWER BY INVOKING CHAIN (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # One line replaces multiple steps of manual prompting\n",
        "\n",
        "    print(f\"\\n[5/5] Invoking RAG chain...\")\n",
        "    print(\"\\n  Invoking chain with context and query...\")\n",
        "    print(\"  The chain will:\")\n",
        "    print(\"    1. Format the prompt template\")\n",
        "    print(\"    2. Send to LLM\")\n",
        "    print(\"    3. Parse response to string\")\n",
        "    print(\"    4. Return answer\")\n",
        "\n",
        "    # Pass variables as dictionary to the chain\n",
        "\n",
        "    # >>>> add code here\n",
        "    response = None\n",
        "\n",
        "    print(f\"\\n✓ Answer generated ({len(response)} characters)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string (v1 returned List[Document])"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Inference"
      ],
      "metadata": {
        "id": "E8bKt_ZRmRC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What is Bitcoin?\"\n",
        "\n",
        "# res = inference(query)\n",
        "\n",
        "# res"
      ],
      "metadata": {
        "id": "xmyUy1AymQrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo"
      ],
      "metadata": {
        "id": "EtDD4ZpKOXmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history):\n",
        "    \"\"\"\n",
        "    Gradio ChatInterface wrapper\n",
        "\n",
        "    Args:\n",
        "        message (str): Current user message\n",
        "        history (list): Chat history (we won't be using it now)\n",
        "\n",
        "    Returns:\n",
        "        str: Bot response\n",
        "    \"\"\"\n",
        "    return inference(message)"
      ],
      "metadata": {
        "id": "Q6A8GuCgOjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=\"Crypto RAG Assistant (v2)\",\n",
        "    description=\"Ask questions about crypto.\",\n",
        "    examples=[\n",
        "        \"What is Bitcoin?\",\n",
        "        \"How does mining work?\",\n",
        "        \"What is proof of work?\",\n",
        "        \"Explain the blockchain structure\",\n",
        "        \"How are transactions verified?\",\n",
        "        \"What is the double-spending problem?\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "R6jjZO2eOvLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}