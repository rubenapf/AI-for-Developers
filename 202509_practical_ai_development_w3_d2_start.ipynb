{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenapf/AI-for-Developers/blob/main/202509_practical_ai_development_w3_d2_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - v2\n",
        "\n",
        "- Same data as v1 (Bitcoin whitepaper)\n",
        "- LLM answer generation using LangChain chain\n",
        "- Gradio ChatInterface\n",
        "- User experience (natural language answers) using also an output parser"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# v2 - added gradio\n",
        "#--------------------------------------------------------------------------------\n",
        "%pip install gradhhjbbbbbbbbio -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200e2025-1f13-47af-b89f-b0d918c2e0b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.2/457.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.3.81 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement gradhhjbbbbbbbbio (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gradhhjbbbbbbbbio\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "\n",
        "# Version Management\n",
        "VERSION = \"v2\"\n",
        "# NOTE: We re-ingest to keep versions clean and independent,\n",
        "#       but technically v2 could use v1's data since structure is identical\n",
        "\n",
        "# Vector Database collection name\n",
        "COLLECTION_NAME = f\"bitcoin_docs_{VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# v2 - added ChatOpenAI\n",
        "#--------------------------------------------------------------------------------\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Vector Database\n",
        "vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=\"code_for_all_rag\"\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingest_documents():\n",
        "    \"\"\"\n",
        "    INGESTION PIPELINE - VERSION v2\n",
        "\n",
        "    DATA: Identical to v1\n",
        "    WHY RE-INGEST? Keep versions independent for teaching clarity and comparison\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"STARTING INGESTION PIPELINE - VERSION {VERSION}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: LOAD DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(\"\\n[1/3] Loading file from URL...\")\n",
        "\n",
        "    loader = PyPDFLoader(\"https://bitcoin.org/bitcoin.pdf\")\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"✓ Loaded {len(documents)} pages from file\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: CHUNK DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/3] Chunking documents...\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"✓ Split into {len(chunks)} chunks\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: CREATE EMBEDDINGS AND STORE IN CHROMA\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[3/3] Creating embeddings and storing in Chroma...\")\n",
        "    print(f\"  Collection name: {COLLECTION_NAME}\")\n",
        "\n",
        "    vectorstore.add_documents(\n",
        "        documents=chunks,\n",
        "        ids=[str(i) for i in range(len(chunks))]\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Embeddings created and stored\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPCOgLJCgHiJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Ingestion"
      ],
      "metadata": {
        "id": "ZSnvBHEnlha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingest_documents()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHP6fpIzlj9B",
        "outputId": "b107cbe5-b410-4f49-aff2-6a70b20bd6a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "STARTING INGESTION PIPELINE - VERSION v2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[1/3] Loading file from URL...\n",
            "✓ Loaded 9 pages from file\n",
            "\n",
            "[2/3] Chunking documents...\n",
            "✓ Split into 30 chunks\n",
            "\n",
            "[3/3] Creating embeddings and storing in Chroma...\n",
            "  Collection name: bitcoin_docs_v2\n",
            "✓ Embeddings created and stored\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Bitcoin?\"\n",
        "\n",
        "res = inference(query)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "xmyUy1AymQrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92017208-e463-4a0d-c25d-bfd0ca1fc6eb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "RUNNING INFERENCE - VERSION v2\n",
            "================================================================================\n",
            "\n",
            "[1/5] Performing similarity search...\n",
            "  Query: 'What is Bitcoin?'\n",
            "✓ Found 3 relevant chunks\n",
            "\n",
            "[2/5] Formatting context for LLM...\n",
            "\n",
            "First 1500 chars: Bitcoin: A Peer-to-Peer Electronic Cash System\n",
            "Satoshi Nakamoto\n",
            "satoshin@gmx.com\n",
            "www.bitcoin.org\n",
            "Abstract.  A purely peer-to-peer version of  electronic cash would allow online  \n",
            "payments to be sent directly from one party to another without going through a  \n",
            "financial institution.  Digital signatures provide part of the solution, but the main  \n",
            "benefits are lost if a trusted third party is still required to prevent double-spending.  \n",
            "We propose a solution to the double-spending problem using a peer-to-peer network. \n",
            "The network timestamps transactions by hashing them into an ongoing chain of  \n",
            "hash-based proof-of-work, forming a record that cannot be changed without redoing  \n",
            "the proof-of-work.  The longest chain not only serves as proof of the sequence of  \n",
            "events witnessed, but proof that it came from the largest pool of CPU power.  As  \n",
            "long as a majority of CPU power is controlled by nodes that are not cooperating to\n",
            "\n",
            "2. Transactions\n",
            "We define an electronic coin as a chain of digital signatures.  Each owner transfers the coin to the  \n",
            "next by digitally signing a hash of the previous transaction and the public key of the next owner  \n",
            "and adding these to the end of the coin.  A payee can verify the signatures to verify the chain of  \n",
            "ownership.\n",
            "The problem of course is the payee can't verify that one of the owners did not double-spend  \n",
            "the coin.  A common solution is to introduce a trusted central authority, or mint, that checks every \n",
            "transaction for double spending.  Af...\n",
            "✓ Context formatted (2831 characters)\n",
            "\n",
            "[3/5] Creating prompt template...\n",
            "  Variables: {context}, {query}\n",
            "✓ Prompt template created\n",
            "  This template will be reused for every query\n",
            "  Variables will be filled automatically by the chain\n",
            "\n",
            "[4/5] Composing chain...\n",
            "\n",
            "✓ Chain composed!\n",
            "\n",
            "  Chain structure:\n",
            "  prompt_template  (formats variables) -> llm (generates response) -> output_parser (extracts string)\n",
            "  Returns: String (natural language answer)\n",
            "\n",
            "[5/5] Invoking RAG chain...\n",
            "\n",
            "  Invoking chain with context and query...\n",
            "  The chain will:\n",
            "    1. Format the prompt template\n",
            "    2. Send to LLM\n",
            "    3. Parse response to string\n",
            "    4. Return answer\n",
            "\n",
            "✓ Answer generated (920 characters)\n",
            "\n",
            "================================================================================\n",
            "INFERENCE COMPLETE\n",
            "================================================================================\n",
            "Bitcoin is a decentralized digital currency system that enables peer-to-peer transactions over the internet without the need for a trusted third party, such as a financial institution. It functions through a network of nodes that maintain a public ledger, or blockchain, which records all transactions. Each transaction is time-stamped and verified using cryptographic techniques, specifically digital signatures, to prevent issues like double-spending. \n",
            "\n",
            "The security of Bitcoin is founded on a proof-of-work mechanism that ensures the integrity of the blockchain by making manipulation computationally impractical, as it would require a majority of the network's computing power to alter any transaction history. This structure allows users to transact directly with one another while relying on the collective effort of the network for verification and trust, effectively creating a decentralized financial ecosystem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def inference(query: str) -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE PIPELINE - VERSION v2\n",
        "\n",
        "    WHAT CHANGED FROM v1?\n",
        "    - Return type: List[Document] → str (natural language answer)\n",
        "    - Added: Context formatting\n",
        "    - Added: Prompt engineering\n",
        "    - Added: LLM generation\n",
        "\n",
        "    THE RAG FLOW:\n",
        "    1. Retrieve: Get relevant documents (same as v1)\n",
        "    2. Format: Combine documents into context string\n",
        "    3. Prompt: Create structured instruction for LLM\n",
        "    4. Chain: prompt -> llm -> output parser\n",
        "    5. Generate: LLM produces natural language answer\n",
        "    6. Return: User gets readable answer\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer (v1 returned List[Document])\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: SIMILARITY SEARCH\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[1/5] Performing similarity search...\")\n",
        "    print(f\"  Query: '{query}'\")\n",
        "\n",
        "    results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    print(f\"✓ Found {len(results)} relevant chunks\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: FORMAT CONTEXT (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[2/5] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "\n",
        "    print(f\"\\nFirst 1500 chars: {context[:1500]}...\")\n",
        "    print(f\"✓ Context formatted ({len(context)} characters)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: PROMPT TEMPLATE (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # ChatPromptTemplate:\n",
        "    # - Defines structure with variables in {curly braces}\n",
        "    # - Variables are filled when chain is invoked\n",
        "    # - Reusable across all queries\n",
        "\n",
        "    print(\"\\n[3/5] Creating prompt template...\")\n",
        "    print(\"  Variables: {context}, {query}\")\n",
        "\n",
        "    prompt_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"Based on the following context, answer the question clearly and concisely (max 2 paragraphs).\"),\n",
        "            (\"human\", \"Documents:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(\"✓ Prompt template created\")\n",
        "    print(\"  This template will be reused for every query\")\n",
        "    print(\"  Variables will be filled automatically by the chain\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: COMPOSE CHAIN (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # The pipe (|) operator connects components (output from the last is the input of the next)\n",
        "    # Read left to right: prompt → llm → parser\n",
        "\n",
        "    # StrOutputParser:\n",
        "    # - LLMs return AIMessage objects (complex)\n",
        "    # - Parser extracts just the string content\n",
        "    # - Clean string output for users\n",
        "\n",
        "    print(\"\\n[4/5] Composing chain...\")\n",
        "\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    print(\"\\n✓ Chain composed!\")\n",
        "    print(\"\\n  Chain structure:\")\n",
        "    print(\"  prompt_template  (formats variables) -> llm (generates response) -> output_parser (extracts string)\")\n",
        "    print(\"  Returns: String (natural language answer)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: GENERATE ANSWER BY INVOKING CHAIN (NEW in v2)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # One line replaces multiple steps of manual prompting\n",
        "\n",
        "    print(f\"\\n[5/5] Invoking RAG chain...\")\n",
        "    print(\"\\n  Invoking chain with context and query...\")\n",
        "    print(\"  The chain will:\")\n",
        "    print(\"    1. Format the prompt template\")\n",
        "    print(\"    2. Send to LLM\")\n",
        "    print(\"    3. Parse response to string\")\n",
        "    print(\"    4. Return answer\")\n",
        "\n",
        "    # Pass variables as dictionary to the chain\n",
        "\n",
        "    response = chain.invoke({\n",
        "        \"context\": context,\n",
        "        \"query\": query\n",
        "    })\n",
        "\n",
        "    print(f\"\\n✓ Answer generated ({len(response)} characters)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string (v1 returned List[Document])"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicised text*### Test Inference"
      ],
      "metadata": {
        "id": "E8bKt_ZRmRC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo"
      ],
      "metadata": {
        "id": "EtDD4ZpKOXmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history):\n",
        "    \"\"\"\n",
        "    Gradio ChatInterface wrapper\n",
        "\n",
        "    Args:\n",
        "        message (str): Current user message\n",
        "        history (list): Chat history (we won't be using it now)\n",
        "\n",
        "    Returns:\n",
        "        str: Bot response\n",
        "    \"\"\"\n",
        "    return inference(message)"
      ],
      "metadata": {
        "id": "Q6A8GuCgOjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=\"Crypto RAG Assistant (v2)\",\n",
        "    description=\"Ask questions about crypto.\",\n",
        "    examples=[\n",
        "        \"What is Bitcoin?\",\n",
        "        \"How does mining work?\",\n",
        "        \"What is proof of work?\",\n",
        "        \"Explain the blockchain structure\",\n",
        "        \"How are transactions verified?\",\n",
        "        \"What is the double-spending problem?\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "R6jjZO2eOvLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}