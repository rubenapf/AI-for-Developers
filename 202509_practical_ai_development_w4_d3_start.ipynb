{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "YceOZZzJZPbC",
        "yV2DhobPawAi",
        "nmxmjjK4c9c7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenapf/AI-for-Developers/blob/main/202509_practical_ai_development_w4_d3_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - v6\n",
        "\n",
        "- Multiquery + Rerank"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup better response formatting (adds line wrap)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wWKinGf0EAmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "%pip install gradio -qqq\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# NEW IN v6: Install FlashRank\n",
        "#--------------------------------------------------------------------------------\n",
        "%pip install flashrank -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "\n",
        "# Langsmith Environment Variables\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Code For All - LangChain Intro\"\n",
        "\n",
        "# Version Management\n",
        "VERSION = \"v6\"\n",
        "\n",
        "# Vector Database collection name\n",
        "COLLECTION_NAME = f\"bitcoin_docs_{VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Classification LLM\n",
        "classification_llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0  # Deterministic for consistent classification\n",
        ")\n",
        "\n",
        "# Vector Database\n",
        "vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=\"code_for_all_rag_1\"\n",
        ")\n",
        "\n",
        "# Parent splitter (large chunks for context)\n",
        "parent_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=240, # 20% of chunk_size\n",
        ")\n",
        "\n",
        "# Child splitter (small chunks for search)\n",
        "# will generate aprox. 4 child documents per parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=60, # 15% of chunk_size\n",
        ")\n",
        "\n",
        "# InMemoryByteStore for parent documents\n",
        "parent_docstore = InMemoryStore()\n",
        "\n",
        "# Langchain Retriever\n",
        "# - Component whose only job is to fetch relevant documents given a query.\n",
        "# - It's like a â€œsearch engineâ€ in your RAG pipeline.\n",
        "\n",
        "# ParentDocumentRetriever\n",
        "# - Stores small chunks in Chroma Cloud (for search)\n",
        "# - Stores large parents in e.g. Google Drive (for better context)\n",
        "parent_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=parent_docstore,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter,\n",
        "    search_kwargs={\"k\": 3}\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean Text Function\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    TEXT PREPROCESSING - Same as v4\n",
        "    \"\"\"\n",
        "    # Remove multiple whitespaces -> \\s+ matches one or more whitespace characters (spaces, tabs, newlines) and replaces them with a single space \" \"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove standalone page numbers -> remove a trailing number only if it's the final token\n",
        "    text = re.sub(r'(?<=\\.)\\s*\\d+\\s*$', '', text)\n",
        "\n",
        "    # Removes whitespace from both the beginning and the end of the string\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "4boHz3P5vKN3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Detection Function\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def detect_document_topic(documents) -> str:\n",
        "    \"\"\"\n",
        "    DETECT DOCUMENT TOPIC - Same as v4\n",
        "    \"\"\"\n",
        "\n",
        "    # Template for automatic topic detection\n",
        "    topic_detection_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Analyze the following document content and determine its primary topic.\n",
        "\n",
        "      Document content:\n",
        "      {content}\n",
        "\n",
        "      Based on this content, what is the primary topic? Answer with a single word or short phrase (e.g., 'bitcoin', 'ethereum').\n",
        "\n",
        "      Examples:\n",
        "      If the document is about Bitcoin, answer: bitcoin\n",
        "      If the document is about Ethereum, answer: ethereum\n",
        "      If the document is about general crypto technology, answer: crypto\n",
        "\n",
        "      Primary topic:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    topic_detection_chain = topic_detection_template | classification_llm | StrOutputParser()\n",
        "\n",
        "    # Extract sample content from first few pages\n",
        "    sample_content = \"\"\n",
        "    for doc in documents[:3]:  # First 3 pages\n",
        "        sample_content += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "    # Limit to 4000 characters to save costs\n",
        "    sample_content = sample_content[:4000]\n",
        "\n",
        "    # Use LLM to detect topic\n",
        "    detected_topic = topic_detection_chain.invoke({\n",
        "        \"content\": sample_content\n",
        "    }).strip().lower()\n",
        "\n",
        "    return detected_topic"
      ],
      "metadata": {
        "id": "BnSC-e9bxgaN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ingest function\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingest_documents(\n",
        "    document_path: str,\n",
        "    access_level: str = \"public\"\n",
        "):\n",
        "    \"\"\"\n",
        "    INGESTION PIPELINE - Same as v5\n",
        "\n",
        "    Parent-Child Document Strategy\n",
        "    1. Add documents with metadata\n",
        "    2. ParentDocumentRetriever automatically:\n",
        "       - Splits into parent chunks (1200 chars)\n",
        "       - Splits parents into child chunks (400 chars)\n",
        "       - Stores children in Chroma Cloud (searchable, persistent)\n",
        "       - Stores parents in InMemoryByteStore (context, session-based)\n",
        "       - Maps children â†’ parents automatically\n",
        "    3. Later retrieval automatically returns parent chunks\n",
        "\n",
        "    Args:\n",
        "        document_path (str): URL or local path to PDF\n",
        "        access_level (str): \"public\" or \"premium\" (for access control demo)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (num_chunks, detected_topic) - Number of chunks and detected topic\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"STARTING INGESTION PIPELINE - VERSION {VERSION}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: LOAD DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(\"\\n[1/5] Loading file from URL...\")\n",
        "\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"âœ“ Loaded {len(documents)} pages from file\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: AUTO-DETECT TOPIC\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/5] Auto-detecting document topic...\")\n",
        "\n",
        "    detected_topic = detect_document_topic(documents)\n",
        "\n",
        "    print(f\"âœ“ Topic auto-detected: '{detected_topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: PREPROCESSING\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/5] Applying text preprocessing...\")\n",
        "\n",
        "    for doc in documents:\n",
        "      doc.page_content = clean_text(doc.page_content)\n",
        "\n",
        "    print(f\"âœ“ Cleaned {len(documents)} pages\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: ADD METADATA (CHANGED IN v5 - no chunks, still docs)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/5] Enriching chunks with metadata...\")\n",
        "\n",
        "    for doc in documents:\n",
        "      # ADD new metadata (doesn't override existing)\n",
        "      doc.metadata.update({\n",
        "          'topic': detected_topic,\n",
        "          'access_level': access_level,   # Used for access control\n",
        "      })\n",
        "\n",
        "    print(f\"âœ“ Metadata enriched for all chunks:\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: USE ParentDocumentRetriever\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[5/5] Processing with ParentDocumentRetriever...\")\n",
        "\n",
        "    # This one line does:\n",
        "    # 1. Split each parent document into child chunks\n",
        "    # 2. Embed and store child chunks in the vectorstore\n",
        "    # 3. Store full parent documents in the docstore\n",
        "    # 4. Link each child chunk to its parent via metadata[\"doc_id\"]\n",
        "    print(f\"Adding {len(documents)} documents/pages (before split)\")\n",
        "    parent_retriever.add_documents(documents)\n",
        "\n",
        "    keys = list(parent_docstore.yield_keys())\n",
        "    print(\"Added\", len(keys), \"documents (after text split) to parent_docstore\")\n",
        "    for key, value in zip(keys, parent_docstore.mget(keys)):\n",
        "        print(\"\\n\" + key, \"=>\", value)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âœ“ Parent chunks created and stored in InMemoryStore\")\n",
        "    print(\"âœ“ Child chunks embeddings created and stored in Chroma\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INGESTION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    return len(documents), detected_topic\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPCOgLJCgHiJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Database Reset (Helper)\n",
        "\n",
        "# Clear InMemoryStore before ingestion\n",
        "print(\"\\nClearing InMemoryStore...\")\n",
        "keys = list(parent_docstore.yield_keys())\n",
        "parent_docstore.mdelete(keys)\n",
        "print(\"InMemoryStore reset! Current size:\", len(list(parent_docstore.yield_keys())))\n",
        "\n",
        "# Reset Chroma collection\n",
        "print(f\"\\nResetting Chroma collection: {COLLECTION_NAME}...\")\n",
        "vectorstore.reset_collection()\n",
        "print(f\"âœ“ Collection '{COLLECTION_NAME}' reset!\")"
      ],
      "metadata": {
        "id": "5JO_PGS_6KLQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute Ingestion\n",
        "\n",
        "ingest_documents(\"https://bitcoin.org/bitcoin.pdf\")\n",
        "#ingest_documents(\"/content/ethereum.pdf\", \"premium\")"
      ],
      "metadata": {
        "id": "VHP6fpIzlj9B",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Classification Chain Creation\n",
        "\n",
        "def create_classification_chain():\n",
        "    \"\"\"\n",
        "    CREATE CLASSIFICATION CHAIN - NEW IN v4 - refactor and nehancement with conversation history\n",
        "\n",
        "    Determines whether a user query is about Bitcoin, Ethereum, or both.\n",
        "    This enables intelligent routing to relevant documents only.\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: Classification chain\n",
        "    \"\"\"\n",
        "\n",
        "    classification_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Classify this question as about 'bitcoin', 'ethereum', or 'both'.\n",
        "      Consider the conversation history for context (e.g., if user previously asked about Bitcoin, \"it\" might refer to Bitcoin).\n",
        "\n",
        "      Conversation history:\n",
        "      {history}\n",
        "\n",
        "      Question: {query}\n",
        "\n",
        "      Classification:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    classification_chain = classification_template | classification_llm | StrOutputParser()\n",
        "\n",
        "    return classification_chain"
      ],
      "metadata": {
        "id": "fdL62p2hDG2f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RAG Chain Creation\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def create_rag_chain():\n",
        "    \"\"\"\n",
        "    CREATE RAG CHAIN - NEW IN v4\n",
        "\n",
        "    KEY DIFFERENCES FROM v3:\n",
        "    - Includes conversation history in the prompt\n",
        "    - Handles follow-up questions\n",
        "    - More natural conversational flow\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: RAG chain\n",
        "    \"\"\"\n",
        "\n",
        "    rag_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      You are a helpful assistant answering questions about cryptocurrency whitepapers.\n",
        "\n",
        "      Use the conversation history and provided documents to answer the current question.\n",
        "      If the question references previous context (e.g., \"it\", \"this\", \"that\"), use the conversation history to understand what is being referenced.\n",
        "\n",
        "      Instructions:\n",
        "      - Answer clearly and concisely (max 2-3 paragraphs)\n",
        "      - Use only the provided documents as context\n",
        "      - If the question is a follow-up, maintain conversation continuity\n",
        "      - If the answer is not in the documents, say \"I don't have enough information to answer that question\"\n",
        "\n",
        "      Conversation History:\n",
        "      {history}\n",
        "\n",
        "      Documents:\n",
        "      {context}\n",
        "\n",
        "      Current Question: {query}\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    rag_chain = rag_template | llm | StrOutputParser()\n",
        "\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "YviF8gfbCMNb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c4620de",
        "cellView": "form"
      },
      "source": [
        "#@title Full RAG Chain Creation\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "def create_parent_child_rag_chain_integrated():\n",
        "    \"\"\"\n",
        "    CREATE INTEGRATED PARENT-CHILD RAG CHAIN - NEW IN v5\n",
        "\n",
        "    This chain combines:\n",
        "    1. Formatting conversation history.\n",
        "    2. Document retrieval using ParentDocumentRetriever.\n",
        "    3. Formatting retrieved documents as context.\n",
        "    4. Generating an answer using the RAG LLM chain.\n",
        "\n",
        "    BENEFITS: One single trace in Langsmith that combines all steps.\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: An integrated RAG chain that takes 'query' and 'chat_history'.\n",
        "    \"\"\"\n",
        "\n",
        "    # The core RAG generation chain (template | LLM | parser) that expects context, query, and history\n",
        "    rag_generation_chain = create_rag_chain()\n",
        "\n",
        "    # Create the full chain that includes retrieval and context formatting\n",
        "    # It takes 'query' and 'chat_history' as initial inputs\n",
        "    full_rag_chain = (\n",
        "        RunnableParallel({\n",
        "            \"context\": (lambda x: x[\"query\"]) | parent_retriever | (lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n",
        "            \"query\": RunnablePassthrough(),\n",
        "            \"history\": (lambda x: format_chat_history(x[\"chat_history\"], max_turns=5)),\n",
        "        })\n",
        "        | rag_generation_chain\n",
        "    )\n",
        "    return full_rag_chain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format Chat History Helper\n",
        "\n",
        "def format_chat_history(history: list, max_turns: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    FORMAT CHAT HISTORY - NEW IN v4\n",
        "\n",
        "    Converts Gradio's messages format history into a readable string.\n",
        "\n",
        "    WHY LIMIT TURNS?\n",
        "    - Token limits: Long conversations exceed context window\n",
        "    - Relevance: Recent turns are more relevant\n",
        "    - Cost: Fewer tokens = lower cost\n",
        "    - Performance: Faster processing\n",
        "\n",
        "    GRADIO MESSAGES FORMAT:\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What is Bitcoin?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Bitcoin is...\"},\n",
        "        {\"role\": \"user\", \"content\": \"How does it work?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"It works by...\"}\n",
        "    ]\n",
        "\n",
        "    Args:\n",
        "        history (list): Gradio chat history in messages format\n",
        "        max_turns (int): Maximum number of conversation turns to include\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted conversation history\n",
        "    \"\"\"\n",
        "\n",
        "    if not history:\n",
        "        return \"No previous conversation.\"\n",
        "\n",
        "    # Limit to last N turns (each turn = user + assistant message)\n",
        "    # Each turn consists of 2 messages (user + assistant)\n",
        "    recent_history = history[-(max_turns * 2):]\n",
        "\n",
        "    # Format as readable conversation\n",
        "    formatted = []\n",
        "    for message in recent_history:\n",
        "        role = message[\"role\"].capitalize()\n",
        "        content = message[\"content\"]\n",
        "        formatted.append(f\"{role}: {content}\")\n",
        "\n",
        "    return \"\\n\".join(formatted)\n"
      ],
      "metadata": {
        "id": "bK3fPBtXgjwi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Basic Inference\n",
        "\n",
        "def inference(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        "    user_access_level: str = \"public\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE PIPELINE - Same as previous versions\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        chat_history (list): Gradio chat history\n",
        "        user_access_level (str): \"public\" or \"premium\"\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY (NEW IN v4)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/6] Formatting conversation history...\")\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history, max_turns=5)\n",
        "\n",
        "    print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: TOPIC DETECTION (CHANGED IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/6] Detecting document topic...\")\n",
        "\n",
        "    topic = create_classification_chain().invoke({\n",
        "        \"query\": query,\n",
        "        \"history\": formatted_history\n",
        "    }).strip().lower()\n",
        "\n",
        "    print(f\"âœ“ Topic: '{topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: METADATA TOPIC FILTER - 2 alternatives: only topic or topic + access_control\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/6] Building metadata filter...\")\n",
        "\n",
        "    filter_conditions = {}\n",
        "\n",
        "    if topic in ['bitcoin', 'ethereum']:\n",
        "        filter_conditions['topic'] = topic\n",
        "        print(f\"  âœ“ Topic filter: {topic}\")\n",
        "\n",
        "    # conditions = []\n",
        "\n",
        "    # if topic in (\"bitcoin\", \"ethereum\"):\n",
        "    #     conditions.append({\"topic\": topic})\n",
        "    #     print(f\"  âœ“ Topic filter: {topic}\")\n",
        "    # else:\n",
        "    #     print(f\"  âœ“ Topic filter: none (searching both)\")\n",
        "\n",
        "    # if user_access_level == \"public\":\n",
        "    #     conditions.append({\"access_level\": \"public\"})\n",
        "    #     print(f\"  âœ“ Access filter: public only\")\n",
        "    # else:\n",
        "    #     print(f\"  âœ“ Access filter: all content\")\n",
        "\n",
        "    # # Combine conditions properly for Chroma\n",
        "    # filter_conditions = {\"$and\": conditions} if len(conditions) > 1 else (conditions[0] if conditions else None)\n",
        "\n",
        "    # print(f\"  Final filter: {filter_conditions or 'None'}\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: SIMILARITY SEARCH\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/6] Performing similarity search...\")\n",
        "\n",
        "    if filter_conditions:\n",
        "        results = vectorstore.similarity_search(query, k=3, filter=filter_conditions)\n",
        "    else:\n",
        "        results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[5/6] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 6: GENERATE ANSWER BY INVOKING CHAIN (CHANGED IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[6/6] Invoking RAG chain...\")\n",
        "\n",
        "    response = create_rag_chain().invoke({\n",
        "        \"context\": context,  # Retrieved documents\n",
        "        \"query\": query,       # User's question\n",
        "        \"history\": formatted_history  # Conversation history\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Retrieval Strategies"
      ],
      "metadata": {
        "id": "qgSs4eVu8pFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title I - Parent-Child Retrieval\n",
        "\n",
        "# =============================================================================\n",
        "# INFERENCE RETRIEVAL STRATEGIES 1: PARENT-CHILD RETRIEVAL\n",
        "# =============================================================================\n",
        "def parent_children_inference(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE WITH PARENT-CHILD RETRIEVAL - Same as v5\n",
        "\n",
        "    Features:\n",
        "    - Uses ParentDocumentRetriever\n",
        "    - Small chunks for search, large chunks for context\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE (PARENT-CHILD) - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"User query: {query}\")\n",
        "    print(f\"Chat history: {chat_history}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/4] Formatting conversation history...\")\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history, max_turns=5)\n",
        "\n",
        "    print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: RETRIEVE WITH ParentDocumentRetriever\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/4] Retrieving with ParentDocumentRetriever...\")\n",
        "\n",
        "    # Get parent documents\n",
        "    results = parent_retriever.invoke(query)\n",
        "\n",
        "    print(f\"âœ“ Retrieved {len(results)} parent documents\")\n",
        "    print(results)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[3/4] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: GENERATE ANSWER BY INVOKING CHAIN\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/4] Invoking RAG chain...\")\n",
        "\n",
        "    response = create_rag_chain().invoke({\n",
        "        \"context\": context,  # Retrieved documents\n",
        "        \"query\": query,       # User's question\n",
        "        \"history\": formatted_history  # Conversation history\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE WITH PARENT-CHILD RETRIEVAL COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "U7KIMnhGoWF_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title I - Parent-Child Retrieval (One Chain)\n",
        "\n",
        "# =============================================================================\n",
        "# INFERENCE RETRIEVAL STRATEGIES 1: PARENT-CHILD RETRIEVAL\n",
        "# =============================================================================\n",
        "def parent_children_inference_one_chain(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE WITH PARENT-CHILD RETRIEVAL (ONE CHAIN) - Same as v5\n",
        "\n",
        "    Features:\n",
        "    - Uses ParentDocumentRetriever\n",
        "    - Small chunks for search, large chunks for context\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE FULL RAG CHAIN - PARENT-CHILD - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"User query: {query}\")\n",
        "    print(f\"Chat history: {chat_history}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nInvoking Full RAG Chain with Parent-Child Retrieval...\")\n",
        "\n",
        "    response = create_parent_child_rag_chain_integrated().invoke({\n",
        "        \"query\": query,       # User's question\n",
        "        \"chat_history\": chat_history  # Pass the original chat_history (list) here\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE WITH PARENT-CHILD RETRIEVAL (ONE CHAIN) COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "4Tv4E5GyFGMR",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title II - Multi-query retrieval + Rerank\n",
        "\n",
        "# =============================================================================\n",
        "# INFERENCE RETRIEVAL STRATEGIES 2: MULTI-QUERY RETRIEVAL + RERANKING\n",
        "# =============================================================================\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain_community.document_compressors import FlashrankRerank\n",
        "\n",
        "def inference_multiquery(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE WITH MULTI-QUERY RETRIEVAL - NEW in v6\n",
        "\n",
        "    MULTI-QUERY:\n",
        "    - LLM generates query variations\n",
        "    - Each variation searches the child chunks\n",
        "    - Combines and retrieves corresponding parent chunks\n",
        "\n",
        "    RERANKING:\n",
        "    - Cross-encoder scores each parent doc against ORIGINAL query\n",
        "    - Reranks all parent documents by relevance\n",
        "    - Keeps only top_n most relevant\n",
        "\n",
        "    TRADE-OFFS:\n",
        "    - Slower than single-strategy approaches (LLM calls for query expansion + reranking)\n",
        "    - More expensive (more tokens processed)\n",
        "    - BUT: Maximum quality for complex/ambiguous queries\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE (MULTI-QUERY) - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"User query: {query}\")\n",
        "    print(f\"Chat history: {chat_history}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/4] Formatting conversation history...\")\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history, max_turns=5)\n",
        "\n",
        "    print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: RETRIEVE WITH MultiQueryRetriever - NEW in v6\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/4] Performing multi-query retrieval...\")\n",
        "\n",
        "    \"\"\"\n",
        "    Creates a MultiQueryRetriever that:\n",
        "    - Generates multiple query variations\n",
        "    - Searches with each variation\n",
        "    - Combines and deduplicates results\n",
        "    \"\"\"\n",
        "\n",
        "    # 2.1 (TO COMPLETE)\n",
        "    # Convert vectorstore to retriever\n",
        "    # base_retriever =\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\nâœ“ Base Retriever documents: {len(results)}\")\n",
        "    print(f\"\\n{results}\")\n",
        "\n",
        "    # 2.2 (TO COMPLETE)\n",
        "    # Create a  multiquery retriever and pass the llm\n",
        "    # multiquery_retriever =\n",
        "\n",
        "    results = []\n",
        "\n",
        "    print(f\"\\nâœ“ Multiquery Retriever documents: {len(results)}\")\n",
        "    print(f\"\\n{results}\")\n",
        "\n",
        "    # 2.3 (TO COMPLETE)\n",
        "    # FlashrankRerank\n",
        "    #\n",
        "    # Cross-encoder is a transformer model: it takes a query and a document_i (encoded together)\n",
        "    # and score how relevant each passage (document content) is to a query.\n",
        "    #\n",
        "    # Setup a compressor\n",
        "    # compressor =\n",
        "\n",
        "\n",
        "    # 2.4 (TO COMPLETE)\n",
        "    # ContextualCompressionRetriever\n",
        "    #\n",
        "    # - Takes standard LangChain Document objects (with .page_content and .metadata).\n",
        "    # - Handles the necessary conversion and adaptation so that external rankers\n",
        "    #   or compressors (like FlashrankRerank, CohereRerank, etc.) can operate on them.\n",
        "    # - Then maps the results back into LangChain Document objects.\n",
        "    #\n",
        "    # compression_retriever =\n",
        "\n",
        "\n",
        "    compressed_docs = []\n",
        "\n",
        "    print(f\"\\nâœ“ Reranked documents: {len(compressed_docs)}\")\n",
        "    print(f\"\\n{compressed_docs}\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[3/4] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in compressed_docs])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: GENERATE ANSWER BY INVOKING CHAIN\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/4] Invoking RAG chain...\")\n",
        "\n",
        "    response = create_rag_chain().invoke({\n",
        "        \"context\": context,  # Retrieved documents\n",
        "        \"query\": query,       # User's question\n",
        "        \"history\": formatted_history  # Conversation history\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE WITH MULTI-QUERY RETRIEVAL COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "7qVBkIEfE42E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Inference\n",
        "\n",
        "#inference(\"What is Bitcoin?\")\n",
        "\n",
        "#parent_children_inference(\"What is Bitcoin?\")\n",
        "\n",
        "inference_multiquery(\"What is Bitcoin?\")"
      ],
      "metadata": {
        "id": "cWN8joPmlXXv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo"
      ],
      "metadata": {
        "id": "EtDD4ZpKOXmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history, retrieval_strategy):\n",
        "  \"\"\"\n",
        "  Gradio ChatInterface wrapper\n",
        "\n",
        "  Args:\n",
        "      message (str): Current user message\n",
        "      history (list): Chat history\n",
        "      retrieval_strategy (str): The selected retrieval strategy (\"basic\", \"parent-child\", \"multi-query\")\n",
        "\n",
        "  Returns:\n",
        "      str: Bot response\n",
        "  \"\"\"\n",
        "  user_access_level = \"premium\"\n",
        "\n",
        "  if retrieval_strategy == \"multi-query\":\n",
        "    return inference_multiquery(\n",
        "        query=message,\n",
        "        chat_history=history\n",
        "    )\n",
        "  elif retrieval_strategy == \"parent-child\":\n",
        "    return parent_children_inference(\n",
        "        query=message,\n",
        "        chat_history=history\n",
        "    )\n",
        "  else: # Default to 'basic' strategy\n",
        "    return inference(\n",
        "        query=message,\n",
        "        chat_history=history,\n",
        "        user_access_level=user_access_level\n",
        "    )"
      ],
      "metadata": {
        "id": "Q6A8GuCgOjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "default_strategy = \"multi-query\"\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=f\"ðŸª™ Bitcoin RAG Assistant ({VERSION})\",\n",
        "    description=\"Ask questions about crypto.\",\n",
        "    examples=[\n",
        "        [\"What is Bitcoin?\", default_strategy],\n",
        "        [\"How does mining work?\", default_strategy],\n",
        "        [\"What is proof of work?\", default_strategy],\n",
        "        [\"Explain the blockchain structure\", default_strategy],\n",
        "        [\"How are transactions verified?\", default_strategy],\n",
        "        [\"What is the double-spending problem?\", default_strategy]\n",
        "    ],\n",
        "    additional_inputs=[\n",
        "        gr.Radio(\n",
        "            [\"basic\", \"parent-child\", \"multi-query\"],\n",
        "            label=\"Retrieval Strategy\",\n",
        "            value=default_strategy, # Default selected value\n",
        "            interactive=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "R6jjZO2eOvLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}