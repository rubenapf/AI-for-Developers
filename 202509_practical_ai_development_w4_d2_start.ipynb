{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenapf/AI-for-Developers/blob/main/202509_practical_ai_development_w4_d2_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - v5\n",
        "\n",
        "- Parent-child document retrival strategy"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "%pip install gradio -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "\n",
        "# Langsmith Environment Variables\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Code For All - LangChain Intro\"\n",
        "\n",
        "# Version Management\n",
        "VERSION = \"v5\"\n",
        "\n",
        "# Vector Database collection name\n",
        "COLLECTION_NAME = f\"bitcoin_docs_{VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#--------------------------------------------------------------------------------\n",
        "# NEW IN v5: Add ParentDocumentRetriever and InMemoryStore\n",
        "#--------------------------------------------------------------------------------\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "#--------------------------------------------------------------------------------\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Classification LLM\n",
        "classification_llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0  # Deterministic for consistent classification\n",
        ")\n",
        "\n",
        "# Vector Database\n",
        "vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=\"code_for_all_rag_1\"\n",
        ")\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# NEW IN v5: Instanciate ParentDocumentRetriever with InMemoryStore\n",
        "#--------------------------------------------------------------------------------\n",
        "\"\"\"\n",
        "PERSISTENCE STRATEGY (in this example):\n",
        "- Child chunks: Chroma Cloud (persistent)\n",
        "- Parent chunks: InMemoryStore (single-session use, persists during Colab session)\n",
        "    - In production use persistent store (Redis, MongoDB, etc.)\n",
        "\"\"\"\n",
        "\n",
        "child_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=60, # 15%\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,\n",
        "    chunk_overlap=240, # 20% of chunk size\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "parent_docstore = InMemoryStore()\n",
        "\n",
        "# Langchain Retriever\n",
        "# - Component whose only job is to fetch relevant documents given a query.\n",
        "# - It's like a â€œsearch engineâ€ in your RAG pipeline.\n",
        "\n",
        "# ParentDocumentRetriever\n",
        "# - Stores small chunks in Chroma Cloud (for search)\n",
        "# - Stores large parents in e.g. Google Drive (for better context)\n",
        "parent_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=parent_docstore,\n",
        "    child_splitter=child_splitter,\n",
        "    parent_splitter=parent_splitter\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean Text Function\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    TEXT PREPROCESSING - Same as v4\n",
        "    \"\"\"\n",
        "    # Remove multiple whitespaces -> \\s+ matches one or more whitespace characters (spaces, tabs, newlines) and replaces them with a single space \" \"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove standalone page numbers -> remove a trailing number only if it's the final token\n",
        "    text = re.sub(r'(?<=\\.)\\s*\\d+\\s*$', '', text)\n",
        "\n",
        "    # Removes whitespace from both the beginning and the end of the string\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "4boHz3P5vKN3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Detection Function\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def detect_document_topic(documents) -> str:\n",
        "    \"\"\"\n",
        "    DETECT DOCUMENT TOPIC - Same as v4\n",
        "    \"\"\"\n",
        "\n",
        "    # Template for automatic topic detection\n",
        "    topic_detection_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Analyze the following document content and determine its primary topic.\n",
        "\n",
        "      Document content:\n",
        "      {content}\n",
        "\n",
        "      Based on this content, what is the primary topic? Answer with a single word or short phrase (e.g., 'bitcoin', 'ethereum').\n",
        "\n",
        "      Examples:\n",
        "      If the document is about Bitcoin, answer: bitcoin\n",
        "      If the document is about Ethereum, answer: ethereum\n",
        "      If the document is about general crypto technology, answer: crypto\n",
        "\n",
        "      Primary topic:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    topic_detection_chain = topic_detection_template | classification_llm | StrOutputParser()\n",
        "\n",
        "    # Extract sample content from first few pages\n",
        "    sample_content = \"\"\n",
        "    for doc in documents[:3]:  # First 3 pages\n",
        "        sample_content += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "    # Limit to 4000 characters to save costs\n",
        "    sample_content = sample_content[:4000]\n",
        "\n",
        "    # Use LLM to detect topic\n",
        "    detected_topic = topic_detection_chain.invoke({\n",
        "        \"content\": sample_content\n",
        "    }).strip().lower()\n",
        "\n",
        "    return detected_topic"
      ],
      "metadata": {
        "id": "BnSC-e9bxgaN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ingest function\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingest_documents(\n",
        "    document_path: str,\n",
        "    access_level: str = \"public\"\n",
        "):\n",
        "    \"\"\"\n",
        "    INGESTION PIPELINE - VERSION v5\n",
        "\n",
        "    NEW IN v5: Parent-Child Document Strategy\n",
        "    1. Add documents with metadata\n",
        "    2. ParentDocumentRetriever automatically:\n",
        "       - Splits into parent chunks (1200 chars)\n",
        "       - Splits parents into child chunks (400 chars)\n",
        "       - Stores children in Chroma Cloud (searchable, persistent)\n",
        "       - Stores parents in InMemoryByteStore (context, session-based)\n",
        "       - Maps children â†’ parents automatically\n",
        "    3. Later retrieval automatically returns parent chunks\n",
        "\n",
        "    Args:\n",
        "        document_path (str): URL or local path to PDF\n",
        "        access_level (str): \"public\" or \"premium\" (for access control demo)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (num_chunks, detected_topic) - Number of chunks and detected topic\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"STARTING INGESTION PIPELINE - VERSION {VERSION}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: LOAD DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(\"\\n[1/5] Loading file from URL...\")\n",
        "\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"âœ“ Loaded {len(documents)} pages from file\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: AUTO-DETECT TOPIC\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/5] Auto-detecting document topic...\")\n",
        "\n",
        "    detected_topic = detect_document_topic(documents)\n",
        "\n",
        "    print(f\"âœ“ Topic auto-detected: '{detected_topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: PREPROCESSING\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/5] Applying text preprocessing...\")\n",
        "\n",
        "    for doc in documents:\n",
        "      doc.page_content = clean_text(doc.page_content)\n",
        "\n",
        "    print(f\"âœ“ Cleaned {len(documents)} pages\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: ADD METADATA (CHANGED IN v5 - no chunks, still docs)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/5] Enriching chunks with metadata...\")\n",
        "\n",
        "    for doc in documents:\n",
        "      # ADD new metadata (doesn't override existing)\n",
        "      doc.metadata.update({\n",
        "          'topic': detected_topic,\n",
        "          'access_level': access_level,   # Used for access control\n",
        "      })\n",
        "\n",
        "    print(f\"âœ“ Metadata enriched for all docs\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: USE ParentDocumentRetriever\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[5/5] Processing with ParentDocumentRetriever...\")\n",
        "\n",
        "    # >> TODO: Add documents with parent_retriever\n",
        "    # This line:\n",
        "    # 1. Split each parent document into child chunks\n",
        "    # 2. Embed and store child chunks in the vectorstore\n",
        "    # 3. Store full parent documents in the doc store\n",
        "    # 4. Link each child chunk to its parent via metadata[\"doc_id\"]\n",
        "    print(f\"Adding {len(documents)} documents to ParentDocumentRetriever...\")\n",
        "    parent_retriever.add_documents(documents)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INGESTION COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    return len(documents), detected_topic\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPCOgLJCgHiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Database Reset (Helper)\n",
        "\n",
        "# Clear InMemoryStore before ingestion\n",
        "print(\"\\nClearing InMemoryStore...\")\n",
        "keys = list(parent_docstore.yield_keys())\n",
        "parent_docstore.mdelete(keys)\n",
        "print(\"InMemoryStore reset! Current size:\", len(list(parent_docstore.yield_keys())))\n",
        "\n",
        "# Reset Chroma collection\n",
        "print(f\"\\nResetting Chroma collection: {COLLECTION_NAME}...\")\n",
        "vectorstore.reset_collection()\n",
        "print(f\"âœ“ Collection '{COLLECTION_NAME}' reset!\")"
      ],
      "metadata": {
        "id": "5JO_PGS_6KLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Execute Ingestion\n",
        "\n",
        "ingest_documents(\"https://bitcoin.org/bitcoin.pdf\")\n",
        "#ingest_documents(\"/content/ethereum.pdf\", \"premium\")"
      ],
      "metadata": {
        "id": "VHP6fpIzlj9B",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Methods"
      ],
      "metadata": {
        "id": "AZEXHOaC8f5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Classification Chain Creation\n",
        "\n",
        "def create_classification_chain():\n",
        "    \"\"\"\n",
        "    CREATE CLASSIFICATION CHAIN - NEW IN v4 - refactor and nehancement with conversation history\n",
        "\n",
        "    Determines whether a user query is about Bitcoin, Ethereum, or both.\n",
        "    This enables intelligent routing to relevant documents only.\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: Classification chain\n",
        "    \"\"\"\n",
        "\n",
        "    classification_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Classify this question as about 'bitcoin', 'ethereum', or 'both'.\n",
        "      Consider the conversation history for context (e.g., if user previously asked about Bitcoin, \"it\" might refer to Bitcoin).\n",
        "\n",
        "      Conversation history:\n",
        "      {history}\n",
        "\n",
        "      Question: {query}\n",
        "\n",
        "      Classification:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    classification_chain = classification_template | classification_llm | StrOutputParser()\n",
        "\n",
        "    return classification_chain"
      ],
      "metadata": {
        "id": "fdL62p2hDG2f",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RAG Chain Creation\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def create_rag_chain():\n",
        "    \"\"\"\n",
        "    CREATE RAG CHAIN - NEW IN v4\n",
        "\n",
        "    KEY DIFFERENCES FROM v3:\n",
        "    - Includes conversation history in the prompt\n",
        "    - Handles follow-up questions\n",
        "    - More natural conversational flow\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: RAG chain\n",
        "    \"\"\"\n",
        "\n",
        "    rag_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      You are a helpful assistant answering questions about cryptocurrency whitepapers.\n",
        "\n",
        "      Use the conversation history and provided documents to answer the current question.\n",
        "      If the question references previous context (e.g., \"it\", \"this\", \"that\"), use the conversation history to understand what is being referenced.\n",
        "\n",
        "      Instructions:\n",
        "      - Answer clearly and concisely (max 2-3 paragraphs)\n",
        "      - Use only the provided documents as context\n",
        "      - If the question is a follow-up, maintain conversation continuity\n",
        "      - If the answer is not in the documents, say \"I don't have enough information to answer that question\"\n",
        "\n",
        "      Conversation History:\n",
        "      {history}\n",
        "\n",
        "      Documents:\n",
        "      {context}\n",
        "\n",
        "      Current Question: {query}\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    rag_chain = rag_template | llm | StrOutputParser()\n",
        "\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "YviF8gfbCMNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c4620de",
        "cellView": "form"
      },
      "source": [
        "#@title Full RAG Chain Creation\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "def create_parent_child_rag_chain_integrated():\n",
        "    \"\"\"\n",
        "    CREATE INTEGRATED PARENT-CHILD RAG CHAIN - NEW IN v5\n",
        "\n",
        "    This chain combines:\n",
        "    1. Formatting conversation history.\n",
        "    2. Document retrieval using ParentDocumentRetriever.\n",
        "    3. Formatting retrieved documents as context.\n",
        "    4. Generating an answer using the RAG LLM chain.\n",
        "\n",
        "    BENEFITS: One single trace in Langsmith that combines all steps.\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: An integrated RAG chain that takes 'query' and 'chat_history'.\n",
        "    \"\"\"\n",
        "\n",
        "    # The core RAG generation chain (template | LLM | parser) that expects context, query, and history\n",
        "    rag_generation_chain = create_rag_chain()\n",
        "\n",
        "    # Create the full chain that includes retrieval and context formatting\n",
        "    # It takes 'query' and 'chat_history' as initial inputs\n",
        "\n",
        "    # >> IMPLEMENT FULL RAG CHAIN\n",
        "    full_rag_chain = None\n",
        "\n",
        "\n",
        "    return full_rag_chain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format Chat History Helper\n",
        "\n",
        "def format_chat_history(history: list, max_turns: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    FORMAT CHAT HISTORY - NEW IN v4\n",
        "\n",
        "    Converts Gradio's messages format history into a readable string.\n",
        "\n",
        "    WHY LIMIT TURNS?\n",
        "    - Token limits: Long conversations exceed context window\n",
        "    - Relevance: Recent turns are more relevant\n",
        "    - Cost: Fewer tokens = lower cost\n",
        "    - Performance: Faster processing\n",
        "\n",
        "    GRADIO MESSAGES FORMAT:\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What is Bitcoin?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Bitcoin is...\"},\n",
        "        {\"role\": \"user\", \"content\": \"How does it work?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"It works by...\"}\n",
        "    ]\n",
        "\n",
        "    Args:\n",
        "        history (list): Gradio chat history in messages format\n",
        "        max_turns (int): Maximum number of conversation turns to include\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted conversation history\n",
        "    \"\"\"\n",
        "\n",
        "    if not history:\n",
        "        return \"No previous conversation.\"\n",
        "\n",
        "    # Limit to last N turns (each turn = user + assistant message)\n",
        "    # Each turn consists of 2 messages (user + assistant)\n",
        "    recent_history = history[-(max_turns * 2):]\n",
        "\n",
        "    # Format as readable conversation\n",
        "    formatted = []\n",
        "    for message in recent_history:\n",
        "        role = message[\"role\"].capitalize()\n",
        "        content = message[\"content\"]\n",
        "        formatted.append(f\"{role}: {content}\")\n",
        "\n",
        "    return \"\\n\".join(formatted)\n"
      ],
      "metadata": {
        "id": "bK3fPBtXgjwi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference Retrival Strategies"
      ],
      "metadata": {
        "id": "qgSs4eVu8pFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.0 Basic Retrival\n",
        "\n",
        "def inference(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        "    user_access_level: str = \"public\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE PIPELINE - VERSION v5 same as v4\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        chat_history (list): Gradio chat history\n",
        "        user_access_level (str): \"public\" or \"premium\"\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/6] Formatting conversation history...\")\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history, max_turns=5)\n",
        "\n",
        "    print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: TOPIC DETECTION\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/6] Detecting document topic...\")\n",
        "\n",
        "    topic = create_classification_chain().invoke({\n",
        "        \"query\": query,\n",
        "        \"history\": formatted_history\n",
        "    }).strip().lower()\n",
        "\n",
        "    print(f\"âœ“ Topic: '{topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: METADATA TOPIC FILTER - 2 alternatives: only topic or topic + access_control\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/6] Building metadata filter...\")\n",
        "\n",
        "    filter_conditions = {}\n",
        "\n",
        "    if topic in ['bitcoin', 'ethereum']:\n",
        "        filter_conditions['topic'] = topic\n",
        "        print(f\"  âœ“ Topic filter: {topic}\")\n",
        "\n",
        "    # conditions = []\n",
        "\n",
        "    # if topic in (\"bitcoin\", \"ethereum\"):\n",
        "    #     conditions.append({\"topic\": topic})\n",
        "    #     print(f\"  âœ“ Topic filter: {topic}\")\n",
        "    # else:\n",
        "    #     print(f\"  âœ“ Topic filter: none (searching both)\")\n",
        "\n",
        "    # if user_access_level == \"public\":\n",
        "    #     conditions.append({\"access_level\": \"public\"})\n",
        "    #     print(f\"  âœ“ Access filter: public only\")\n",
        "    # else:\n",
        "    #     print(f\"  âœ“ Access filter: all content\")\n",
        "\n",
        "    # # Combine conditions properly for Chroma\n",
        "    # filter_conditions = {\"$and\": conditions} if len(conditions) > 1 else (conditions[0] if conditions else None)\n",
        "\n",
        "    # print(f\"  Final filter: {filter_conditions or 'None'}\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: SIMILARITY SEARCH\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/6] Performing similarity search...\")\n",
        "\n",
        "    if filter_conditions:\n",
        "        results = vectorstore.similarity_search(query, k=3, filter=filter_conditions)\n",
        "    else:\n",
        "        results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[5/6] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 6: GENERATE ANSWER BY INVOKING CHAIN\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[6/6] Invoking RAG chain...\")\n",
        "\n",
        "    response = create_rag_chain().invoke({\n",
        "        \"context\": context,  # Retrieved documents\n",
        "        \"query\": query,       # User's question\n",
        "        \"history\": formatted_history  # Conversation history\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.0 Parent-Child Retrival\n",
        "\n",
        "def parent_children_inference(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE WITH PARENT-CHILD RETRIEVAL - NEW IN v5\n",
        "\n",
        "    Features:\n",
        "    - Uses ParentDocumentRetriever\n",
        "    - Small chunks for search, large chunks for context\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE (PARENT-CHILD) - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"User query: {query}\")\n",
        "    print(f\"Chat history: {chat_history}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/4] Formatting conversation history...\")\n",
        "\n",
        "    formatted_history = format_chat_history(chat_history, max_turns=5)\n",
        "\n",
        "    print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: RETRIEVE WITH ParentDocumentRetriever - NEW in V5\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/4] Retrieving with ParentDocumentRetriever...\")\n",
        "\n",
        "    # >> TODO: Get parent documents\n",
        "    results = \"\"\n",
        "\n",
        "    print(f\"âœ“ Retrieved {len(results)} parent documents\")\n",
        "    print(results)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[3/4] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: GENERATE ANSWER BY INVOKING CHAIN\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/4] Invoking RAG chain...\")\n",
        "\n",
        "    response = create_rag_chain().invoke({\n",
        "        \"context\": context,  # Retrieved documents\n",
        "        \"query\": query,       # User's question\n",
        "        \"history\": formatted_history  # Conversation history\n",
        "    })\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "U7KIMnhGoWF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.5 Parent-Child Retrival Full Chain\n",
        "\n",
        "def parent_children_inference_full_chain(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE WITH PARENT-CHILD RETRIEVAL FULL CHAIN - NEW IN v5\n",
        "\n",
        "    Features:\n",
        "    - Uses ParentDocumentRetriever\n",
        "    - Small chunks for search, large chunks for context\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE FULL RAG CHAIN - PARENT-CHILD - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"User query: {query}\")\n",
        "    print(f\"Chat history: {chat_history}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\\nInvoking Full RAG Chain with Parent-Child Retrieval...\")\n",
        "\n",
        "    # >> TODO: Complete Chain\n",
        "    response = None\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "4Tv4E5GyFGMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(\"What is Bitcoin?\")\n",
        "\n",
        "#parent_children_inference(\"What is Bitcoin?\")"
      ],
      "metadata": {
        "id": "cWN8joPmlXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo"
      ],
      "metadata": {
        "id": "EtDD4ZpKOXmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history):\n",
        "  \"\"\"\n",
        "  Gradio ChatInterface wrapper\n",
        "\n",
        "  Args:\n",
        "      message (str): Current user message\n",
        "      history (list): Chat history (we won't be using it now)\n",
        "\n",
        "  Returns:\n",
        "      str: Bot response\n",
        "  \"\"\"\n",
        "  user_access_level = \"premium\"\n",
        "\n",
        "  # return inference(\n",
        "  #     query=message,\n",
        "  #     chat_history=history,\n",
        "  #     user_access_level=user_access_level\n",
        "  # )\n",
        "\n",
        "  # >> TODO: Use parent_children_inference\n",
        "\n"
      ],
      "metadata": {
        "id": "Q6A8GuCgOjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=\"ðŸª™ Bitcoin RAG Assistant (v2)\",\n",
        "    description=\"Ask questions about crypto.\",\n",
        "    examples=[\n",
        "        \"What is Bitcoin?\",\n",
        "        \"How does mining work?\",\n",
        "        \"What is proof of work?\",\n",
        "        \"Explain the blockchain structure\",\n",
        "        \"How are transactions verified?\",\n",
        "        \"What is the double-spending problem?\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "R6jjZO2eOvLm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}