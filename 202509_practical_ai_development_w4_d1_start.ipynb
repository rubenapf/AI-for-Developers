{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubenapf/AI-for-Developers/blob/main/202509_practical_ai_development_w4_d1_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG - v4\n",
        "\n",
        "- Refactor Ingestion\n",
        "  - Cleaning\n",
        "  - Topic Detection\n",
        "- Refactor Inference\n",
        "  - Topic Classification\n",
        "  - RAG Chain\n",
        "- Gradio Chat History\n",
        "  - format messages function"
      ],
      "metadata": {
        "id": "DDEMjOnRY_i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "YceOZZzJZPbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"langchain==0.3.27\" -qqq\n",
        "%pip install \"langchain-community==0.3.31\" -qqq\n",
        "%pip install \"langchain-openai==0.3.35\" -qqq\n",
        "%pip install \"langchain-chroma==0.2.6\" -qqq\n",
        "%pip install pypdf -qqq\n",
        "%pip install gradio -qqq"
      ],
      "metadata": {
        "id": "0h03ZTtdZUop",
        "collapsed": true,
        "outputId": "1a860f33-2886-4d35-8f37-6bf82aeadacd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/457.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m450.6/457.2 kB\u001b[0m \u001b[31m155.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m457.2/457.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langgraph-prebuilt 1.0.5 requires langchain-core>=1.0.0, but you have langchain-core 0.3.81 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "id": "yV2DhobPawAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"CHROMA_API_KEY\"] = userdata.get(\"CHROMA_API_KEY\")\n",
        "os.environ[\"CHROMA_TENANT\"] = userdata.get(\"CHROMA_TENANT\")\n",
        "\n",
        "#langsmith\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-v4\"\n",
        "\n",
        "# Version Management\n",
        "VERSION = \"v4\"\n",
        "\n",
        "# Vector Database collection name\n",
        "COLLECTION_NAME = f\"bitcoin_docs_{VERSION}\"\n"
      ],
      "metadata": {
        "id": "ReJeaLCWa1RZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Global Variables"
      ],
      "metadata": {
        "id": "nmxmjjK4c9c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Embeddings Model\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# Classification LLM\n",
        "classification_llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0  # Deterministic for consistent classification\n",
        ")\n",
        "\n",
        "# Vector Database\n",
        "vectorstore = Chroma(\n",
        "  embedding_function=embeddings,\n",
        "  collection_name=COLLECTION_NAME,  # Version-based naming\n",
        "  chroma_cloud_api_key=os.getenv(\"CHROMA_API_KEY\"),\n",
        "  tenant=os.getenv(\"CHROMA_TENANT\"),\n",
        "  database=\"code_for_all_rag\"\n",
        ")"
      ],
      "metadata": {
        "id": "SZmapQgqc9z0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ingestion"
      ],
      "metadata": {
        "id": "hXssSQf8gCiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clean Text Function\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    TEXT PREPROCESSING - Clean PDF - NEW IN v4\n",
        "\n",
        "    Args:\n",
        "        text (str): Raw PDF text\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned text\n",
        "    \"\"\"\n",
        "    # Remove multiple whitespaces -> \\s+ matches one or more whitespace characters (spaces, tabs, newlines) and replaces them with a single space \" \"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Remove standalone page numbers -> remove a trailing number only if it's the final token\n",
        "    text = re.sub(r'(?<=\\.)\\s*\\d+\\s*$', '', text)\n",
        "\n",
        "    # Removes whitespace from both the beginning and the end of the string\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "4boHz3P5vKN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Detection Function\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def detect_document_topic(documents) -> str:\n",
        "    \"\"\"\n",
        "    DETECT DOCUMENT TOPIC - NEW IN v4\n",
        "\n",
        "    GOAL: Wrapper arround V3 functions\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of Document objects from PyPDFLoader\n",
        "\n",
        "    Returns:\n",
        "        str: Detected topic (e.g., 'bitcoin', 'ethereum')\n",
        "    \"\"\"\n",
        "\n",
        "    # Template for automatic topic detection\n",
        "    topic_detection_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Analyze the following document content and determine its primary topic.\n",
        "\n",
        "      Document content:\n",
        "      {content}\n",
        "\n",
        "      Based on this content, what is the primary topic? Answer with a single word or short phrase (e.g., 'bitcoin', 'ethereum').\n",
        "\n",
        "      Examples:\n",
        "      If the document is about Bitcoin, answer: bitcoin\n",
        "      If the document is about Ethereum, answer: ethereum\n",
        "      If the document is about general crypto technology, answer: crypto\n",
        "\n",
        "      Primary topic:\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    topic_detection_chain = topic_detection_template | classification_llm | StrOutputParser()\n",
        "\n",
        "    # Extract sample content from first few pages\n",
        "    sample_content = \"\"\n",
        "    for doc in documents[:3]:  # First 3 pages\n",
        "        sample_content += doc.page_content + \"\\n\\n\"\n",
        "\n",
        "    # Limit to 4000 characters to save costs\n",
        "    sample_content = sample_content[:4000]\n",
        "\n",
        "    # Use LLM to detect topic\n",
        "    detected_topic = topic_detection_chain.invoke({\n",
        "        \"content\": sample_content\n",
        "    }).strip().lower()\n",
        "\n",
        "    return detected_topic"
      ],
      "metadata": {
        "id": "d_5t7uQCuXc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ingest function\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def ingest_documents(\n",
        "    document_path: str,\n",
        "    access_level: str = \"public\"\n",
        "):\n",
        "    \"\"\"\n",
        "    INGESTION PIPELINE - VERSION v4\n",
        "\n",
        "    Args:\n",
        "        document_path (str): URL or local path to PDF\n",
        "        access_level (str): \"public\" or \"premium\" (for access control demo)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (num_chunks, detected_topic) - Number of chunks and detected topic\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"STARTING INGESTION PIPELINE - VERSION {VERSION}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: LOAD DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(\"\\n[1/6] Loading file from URL...\")\n",
        "\n",
        "    loader = PyPDFLoader(document_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    print(f\"âœ“ Loaded {len(documents)} pages from file\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: AUTO-DETECT TOPIC (TO DO IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/6] Auto-detecting document topic...\")\n",
        "\n",
        "    detected_topic = \"\"\n",
        "    #  >> TO DO IN v4: Use detect_document_topic()\n",
        "\n",
        "    print(f\"âœ“ Topic auto-detected: '{detected_topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: PREPROCESSING (TO DO IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/6] Applying text preprocessing...\")\n",
        "\n",
        "    # print(documents[0].page_content)\n",
        "\n",
        "    # >> TO DO IN v4: Use clean_text()\n",
        "\n",
        "    # print(f\"\\n{documents[0].page_content}\")\n",
        "\n",
        "    print(f\"âœ“ Cleaned {len(documents)} pages\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: CHUNK DOCUMENTS\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/6] Chunking documents...\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        # separators=[\"\\n\\n\", \"\\n\",\" \", \"\"], # added dot + space to the list of separators\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "    print(f\"âœ“ Split into {len(chunks)} chunks\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: ADD METADATA\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[5/6] Enriching chunks with metadata...\")\n",
        "\n",
        "    for chunk in chunks:\n",
        "      # ADD new metadata (doesn't override existing)\n",
        "      chunk.metadata.update({\n",
        "          'topic': detected_topic,\n",
        "          'access_level': access_level,   # Used for access control\n",
        "      })\n",
        "\n",
        "    print(f\"âœ“ Metadata enriched for all chunks:\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 6: CREATE EMBEDDINGS AND STORE IN CHROMA\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[6/6] Creating embeddings and storing in Chroma...\")\n",
        "    print(f\"  Collection name: {COLLECTION_NAME}\")\n",
        "\n",
        "    vectorstore.add_documents(\n",
        "        documents=chunks\n",
        "    )\n",
        "\n",
        "    print(f\"âœ“ Embeddings created and stored\")\n",
        "\n",
        "    return len(chunks), detected_topic\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPCOgLJCgHiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Ingestion"
      ],
      "metadata": {
        "id": "ZSnvBHEnlha8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingest_documents(\"https://bitcoin.org/bitcoin.pdf\")\n",
        "\n",
        "ingest_documents(\"/content/ethereum.pdf\", \"premium\")"
      ],
      "metadata": {
        "id": "VHP6fpIzlj9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "61s-geB4kdqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Topic Classification Chain Creation\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "def create_classification_chain() -> RunnableSequence:\n",
        "    \"\"\"\n",
        "    CREATE CLASSIFICATION CHAIN - TO DO IN v4\n",
        "\n",
        "    GOAL: Refactor and enhance with conversation history\n",
        "\n",
        "    OUTCOME:\n",
        "     - Determine whether a user query is about Bitcoin, Ethereum, or both.\n",
        "     - Enable intelligent routing to relevant documents only.\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: Classification chain\n",
        "    \"\"\"\n",
        "\n",
        "    classification_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      Classify this question as about 'bitcoin', 'ethereum', or 'both'.\n",
        "\n",
        "      >>> Complete Prompt Template\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "     ## >>> Complete classification Chain\n",
        "\n",
        "    classification_chain = None\n",
        "\n",
        "    return classification_chain"
      ],
      "metadata": {
        "id": "fdL62p2hDG2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RAG Chain Creation\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "def create_rag_chain() -> RunnableSequence:\n",
        "    \"\"\"\n",
        "    CREATE RAG CHAIN - TO DO IN v4\n",
        "\n",
        "    KEY DIFFERENCES WE MUST INCLUDE FROM v3:\n",
        "    - Conversation history in the prompt\n",
        "    - Handleing for follow-up questions\n",
        "\n",
        "    OUTCOME:\n",
        "    - More natural conversational flow\n",
        "\n",
        "    Returns:\n",
        "        RunnableSequence: RAG chain\n",
        "    \"\"\"\n",
        "\n",
        "    rag_template = ChatPromptTemplate.from_template(\n",
        "      \"\"\"\n",
        "      You are a helpful assistant answering questions about cryptocurrency whitepapers.\n",
        "\n",
        "      >>> Complete Prompt Template\n",
        "      \"\"\"\n",
        "    )\n",
        "\n",
        "    ## >>> Complete RAG Chain\n",
        "\n",
        "    rag_chain = None\n",
        "\n",
        "    return rag_chain"
      ],
      "metadata": {
        "id": "YviF8gfbCMNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Format Chat History Helper\n",
        "\n",
        "def format_chat_history(history: list, max_turns: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    FORMAT CHAT HISTORY - TO DO IN v4\n",
        "\n",
        "    GOAL: Convert Gradio's messages format history (Args) into a string (Return).\n",
        "\n",
        "    WHY LIMIT TURNS?\n",
        "    - Token limits: Long conversations exceed context window\n",
        "    - Relevance: Recent turns are more relevant\n",
        "    - Cost: Fewer tokens = lower cost\n",
        "    - Performance: Faster processing\n",
        "\n",
        "    GRADIO MESSAGES FORMAT:\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What is Bitcoin?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Bitcoin is...\"},\n",
        "        {\"role\": \"user\", \"content\": \"How does it work?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"It works by...\"}\n",
        "    ]\n",
        "\n",
        "    Args:\n",
        "        history (list): Gradio chat history in messages format\n",
        "        max_turns (int): Maximum number of conversation turns to include\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted conversation history\n",
        "    \"\"\"\n",
        "\n",
        "    # >> Add rest of the code\n",
        "\n",
        "    formatted_history = \"\"\n",
        "\n",
        "    return formatted_history"
      ],
      "metadata": {
        "id": "bK3fPBtXgjwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Inference Function\n",
        "\n",
        "def inference(\n",
        "    query: str,\n",
        "    chat_history: list = None,\n",
        "    user_access_level: str = \"public\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    INFERENCE PIPELINE - VERSION v4\n",
        "\n",
        "    FLOW:\n",
        "    1. Format conversation history\n",
        "    2. Classify query as topic (using a topic classification chain)\n",
        "    3. Build metadata filter (topic + access_level)\n",
        "    4. Retrieve filtered documents (using a chain)\n",
        "    5. Generate answer\n",
        "\n",
        "    KEY CHANGES FROM v3:\n",
        "    - Uses chat_history for context\n",
        "    - Classification considers history\n",
        "    - RAG chain includes conversation history\n",
        "    - Handles follow-up questions\n",
        "\n",
        "    Args:\n",
        "        query (str): User's question\n",
        "        chat_history (list): Gradio chat history\n",
        "        user_access_level (str): \"public\" or \"premium\"\n",
        "\n",
        "    Returns:\n",
        "        str: Natural language answer\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUNNING INFERENCE - VERSION {VERSION}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 1: FORMAT CONVERSATION HISTORY (TO DO IN v4)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[1/6] Formatting conversation history...\")\n",
        "\n",
        "    # >>> Implement format_chat_history()\n",
        "    formatted_history = \"\"\n",
        "\n",
        "    # print(f\"âœ“ History formatted ({len(chat_history or [])} messages)\")\n",
        "    # print(f\"\\n{formatted_history}\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 2: TOPIC DETECTION (TO DO IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[2/6] Detecting document topic...\")\n",
        "\n",
        "    # >>> Implement create_classification_chain()\n",
        "    topic = \"bitcoin\"\n",
        "\n",
        "    # print(f\"âœ“ Topic: '{topic}'\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 3: METADATA TOPIC FILTER - 2 alternatives: only topic or topic + access_control\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[3/6] Building metadata filter...\")\n",
        "\n",
        "    # Filter: only topic\n",
        "    # filter_conditions = {}\n",
        "\n",
        "    # if topic in ['bitcoin', 'ethereum']:\n",
        "    #     filter_conditions['topic'] = topic\n",
        "    #     print(f\"  âœ“ Topic filter: {topic}\")\n",
        "\n",
        "\n",
        "    # Filter: topic + access_control\n",
        "    conditions = []\n",
        "\n",
        "    if topic in (\"bitcoin\", \"ethereum\"):\n",
        "        conditions.append({\"topic\": topic})\n",
        "        print(f\"  âœ“ Topic filter: {topic}\")\n",
        "    else:\n",
        "        print(f\"  âœ“ Topic filter: none (searching both)\")\n",
        "\n",
        "    if user_access_level == \"public\":\n",
        "        conditions.append({\"access_level\": \"public\"})\n",
        "        print(f\"  âœ“ Access filter: public only\")\n",
        "    else:\n",
        "        print(f\"  âœ“ Access filter: all content\")\n",
        "\n",
        "    # Combine conditions properly for Chroma\n",
        "    filter_conditions = {\"$and\": conditions} if len(conditions) > 1 else (conditions[0] if conditions else None)\n",
        "\n",
        "    print(f\"  Final filter: {filter_conditions or 'None'}\")\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 4: SIMILARITY SEARCH\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[4/6] Performing similarity search...\")\n",
        "\n",
        "    if filter_conditions:\n",
        "        results = vectorstore.similarity_search(query, k=3, filter=filter_conditions)\n",
        "    else:\n",
        "        results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 5: FORMAT CONTEXT\n",
        "    #--------------------------------------------------------------------------------\n",
        "\n",
        "    print(f\"\\n[5/6] Formatting context for LLM...\")\n",
        "\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    #--------------------------------------------------------------------------------\n",
        "    # STEP 6: GENERATE ANSWER BY INVOKING CHAIN (TO DO IN v4 - refactor)\n",
        "    #--------------------------------------------------------------------------------\n",
        "    print(f\"\\n[6/6] Invoking RAG chain...\")\n",
        "\n",
        "    # >>> Implement create_rag_chain method\n",
        "    response = \"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"INFERENCE COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return response  # Returns string"
      ],
      "metadata": {
        "id": "JE72OpGlkfCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference(\"What is Bitcoin?\")"
      ],
      "metadata": {
        "id": "cWN8joPmlXXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Demo"
      ],
      "metadata": {
        "id": "EtDD4ZpKOXmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_inference(message, history):\n",
        "  print(history)\n",
        "  \"\"\"\n",
        "  Gradio ChatInterface wrapper\n",
        "\n",
        "  Args:\n",
        "      message (str): Current user message\n",
        "      history (list): Chat history (we won't be using it now)\n",
        "\n",
        "  Returns:\n",
        "      str: Bot response\n",
        "  \"\"\"\n",
        "  user_access_level = \"premium\"\n",
        "\n",
        "  return inference(\n",
        "      query=message,\n",
        "      chat_history=history,\n",
        "      user_access_level=user_access_level\n",
        "  )"
      ],
      "metadata": {
        "id": "Q6A8GuCgOjuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_inference,\n",
        "    type=\"messages\",\n",
        "    title=\"ğŸª™ Bitcoin RAG Assistant (v2)\",\n",
        "    description=\"Ask questions about crypto.\",\n",
        "    examples=[\n",
        "        \"What is Bitcoin?\",\n",
        "        \"What is Ethereum?\",\n",
        "        \"How does mining work?\",\n",
        "        \"What is proof of work?\",\n",
        "        \"Explain the blockchain structure\",\n",
        "        \"How are transactions verified?\",\n",
        "        \"What is the double-spending problem?\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "R6jjZO2eOvLm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}